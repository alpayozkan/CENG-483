\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[dvips]{graphicx}
\usepackage{epsfig}
\usepackage{fancybox}
\usepackage{verbatim}
\usepackage{array}
\usepackage{latexsym}
\usepackage{alltt}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[hmargin=3cm,vmargin=5.0cm]{geometry}
\usepackage{epstopdf}
\topmargin=-1.8cm
\addtolength{\textheight}{6.5cm}
\addtolength{\textwidth}{2.0cm}
\setlength{\oddsidemargin}{0.0cm}
\setlength{\evensidemargin}{0.0cm}
\newcommand{\HRule}{\rule{\linewidth}{1mm}}
\newcommand{\kutu}[2]{\framebox[#1mm]{\rule[-2mm]{0mm}{#2mm}}}
\newcommand{\gap}{ \\[1mm] }
\newcommand{\Q}{\raisebox{1.7pt}{$\scriptstyle\bigcirc$}}
\newcommand{\minus}{\scalebox{0.35}[1.0]{$-$}}



\lstset{
    %backgroundcolor=\color{lbcolor},
    tabsize=2,
    language=MATLAB,
    basicstyle=\footnotesize,
    numberstyle=\footnotesize,
    aboveskip={0.0\baselineskip},
    belowskip={0.0\baselineskip},
    columns=fixed,
    showstringspaces=false,
    breaklines=true,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    %frame=single,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    identifierstyle=\ttfamily,
    keywordstyle=\color[rgb]{0,0,1},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
}


\begin{document}

\noindent
\HRule %\\[3mm]
\small
\begin{center}
    \LARGE \textbf{CENG 483} \\[4mm]
    \Large Introduction to Computer Vision \\[4mm]
    \normalsize Fall 2021-2022 \\
    \Large Take Home Exam 2 \\
    \Large Object Recognition \\
    \Large Student Random ID: \\
\end{center}
\HRule

\begin{center}
\end{center}
\vspace{-10mm}
\noindent\\ \\ 
Please fill in the sections below only with the requested information. If you have additional things to mention, you can use the last section. Please note that all of the results in this report should be given for the \textbf{validation set}. Also, when you are expected to comment on the effect of a parameter, please make sure to fix other parameters.

\section{Local Features (25 pts)}
    \begin{itemize}
        \item Explain SIFT and Dense-SIFT in your own words. What is the main difference?
        
        \item Put your quantitative results (classification accuracy) regarding 5 values of SIFT and 3 values of Dense-SIFT parameters here. In SIFT change each parameter once while keeping others same and in Dense-SIFT change size of feature extraction region. Discuss the effect of these parameters by using 128 clusters in k-means and 8 nearest neighbors for classification.
        
    \end{itemize}
    
    \vspace*{0.5cm}
    \begin{center}
        \raggedright
        Sift is a keypoint detector that is designed to capture features that are indepent from the scale of the image.
        Its scale invariance property stems from difference of gaussian operations with different kernel size resolutions which captures keypoints at different scales.
        Then we can quantize an orientation for each keypoint and when orientations of all features are described wrt the keypoint orientation, then it'll additionally have rotation invariance alongside scale invariance.
        These are all desired invariances that are needed to select interesting or descriptive points in images in order to identify, match, classify etc.
        Dense-Sift is a kind of modification on top of conventional Sift method. In Sift locations of keypoint descriptors are determined by the default algorithm.
        On the contrary, in Dense-Sift center-positions (x,y) can be fed to Sift algorithm to create descriptors around that point, or equivalently one can partition the image into grid like portions 
        and feed them individually through default Sift which in a way has similar behaviour, resulting in specific sift vectors around that spatial region.
    \end{center}
    
    \vspace*{0.5cm}
        \begin{tabular}{ |p{1.5cm}||p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{2cm}|  }
            \hline
            \multicolumn{6}{|c|}{Sift: Parameters vs Accuracy (default parameters)} \\
            \hline
            Accuracy (1.0) & nfeatures & nOctaveLayers & contrastThreshold & edgeThreshold & sigma \\
            \hline
            0.17 & 0 & 3 & 0.04 & 10 & 1.6 \\
            \hline
            - & - & - & - & - & - \\
            \hline
            - & - & - & - & - & - \\
            \hline
            - & - & - & - & - & - \\
            \hline
            - & - & - & - & - & - \\
            \hline
        \end{tabular}
    
        \vspace*{0.5cm}
    \begin{tabular}{ |p{1.5cm}||p{2cm}| }
        \hline
        \multicolumn{2}{|c|}{Sift: Parameters vs Accuracy (other params default)} \\
        \hline
        Accuracy (1.0) & nfeatures \\
        \hline
        0.17 & 0 \\
        \hline
        0.17 & 100 \\
        \hline
        0.17 & 20 \\
        \hline
        0.16 & 10 \\
        \hline
        0.10 & 1 \\
        \hline
    \end{tabular}

    \vspace*{0.5cm}
    \begin{tabular}{ |p{1.5cm}||p{2cm}| }
        \hline
        \multicolumn{2}{|c|}{Sift: Parameters vs Accuracy (other params default)} \\
        \hline
        Accuracy (1.0) & nOctaveLayers \\
        \hline
        0.19 & 1 \\
        \hline
        0.19 & 2 \\
        \hline
        0.17 & 3 \\
        \hline
        0.18 & 5 \\
        \hline
        0.17 & 7 \\
        \hline
    \end{tabular}

    \vspace*{0.5cm}
    \begin{tabular}{ |p{1.5cm}||p{2cm}| }
        \hline
        \multicolumn{2}{|c|}{Sift: Parameters vs Accuracy (other params default)} \\
        \hline
        Accuracy (1.0) & contrastThreshold \\
        \hline
        0.0 & 1 \\
        \hline
        0.22 & 0.1 \\
        \hline
        0.16 & 0.04 \\
        \hline
        0.16 & 0.01 \\
        \hline
        0.17 & 0.001 \\
        \hline
    \end{tabular}

    \vspace*{0.5cm}
    \begin{tabular}{ |p{1.5cm}||p{2cm}| }
        \hline
        \multicolumn{2}{|c|}{Sift: Parameters vs Accuracy (other params default)} \\
        \hline
        Accuracy (1.0) & edgeThreshold \\
        \hline
        0 & 1 \\
        \hline
        0.18 & 10 \\
        \hline
        0.18 & 50 \\
        \hline
        0.2 & 100 \\
        \hline
        0.19 & 200 \\
        \hline
        0.18 & 300 \\
        \hline
    \end{tabular}

    \vspace*{0.5cm}
    \begin{tabular}{ |p{1.5cm}||p{2cm}| }
        \hline
        \multicolumn{2}{|c|}{Sift: Parameters vs Accuracy (other params default)} \\
        \hline
        Accuracy (1.0) & sigma \\
        \hline
        0.16 & 1 \\
        \hline
        0.17 & 1.6 \\
        \hline
        0.18 & 2 \\
        \hline
        0.19 & 3 \\
        \hline
    \end{tabular}

    \vspace*{0.5cm}
        \begin{tabular}{ |p{1.5cm}||p{3cm}|  }
            \hline
            \multicolumn{2}{|c|}{Dense-Sift: Parameters vs Accuracy} \\
            \hline
            Accuracy (1.0) & Grid\_Size \\
            \hline
            - & - \\
            \hline
            - & - \\
            \hline
            - & - \\
            \hline
        \end{tabular}

    \begin{center}
        \raggedright
        In this experiment, k-value in kmeans is set to 128 cluster and k value of k\_nn neearest neighbor is set to 8 as required.
        By changing the parameters of SIFT, I've obtained significant changes on the accuracy of our classifier.
        \\~\\
        Firstly, nfeatures determines the number of top features in terms of local contrast scores to keep in the sift descriptor. 
        Default nfeatures value takes all of the features in the descriptor. When nfeatures is set very low like 1, I've observed that accuracy drops drastically
        which makes sense because we are dropping discriminative sift vectors and this information loss has a cost on the overall accuracy.
        However, nfeatures=20 or 100, more or less yields the same accuracy which probably means that top 20 sift vectors were discriminative enough to make 
        classification equivalent to the default param that is used in sift.
        \\~\\
        Secondly, nOctaveLayers are number of difference of gaussian layers which helps capturing features at varying scales.
        If we have more nOctaveLayers then that would be more features at varying scales but with exponentially (with order of $k^{2}$) more blurring.
        This creates new features at very high scale due to higher blurring as a result of increased nOctaveLayers value.
        As far as I understand, in sift they have probably chosen the elbow value which should be somewhere around 3, because after/smaller than 3 it starts to decline very sharply.
        In addition, having less nOctaveLayers might be disadvantageous too depending on the problem and image properties, hence I think 3 would be a better or less risky choice when compared to 1,2 which results
        in less scale invariant descriptors.
        \\~\\
        Thirdly, contrastThreshold parameter filters weak feature below a threshold value measured in contrast.
        When it's set higher, we would get less features and when it's set lower, we would get more features.
        However, more features don't always imply more accuracy, some redundant features can hurt our performance in classification as we'll see.
        I've observed that increasing contrastThreshold, results in less sift descriptors. For instance, contrastThreshold=1 results no sift vector, because it's too high and contrastThreshold=0.001 results in most number of sift descriptors.
        ContrastThreshold is a hyperparameter that we should tune for our problem and in our case 0.1 has resulted in the best accuracy, because it has discarded less important features and emphasized on more important features.
        \\~\\
        Fourtly, edgeThreshold is a hyperparameter that basically filters edge like features, its meaning is slightly different than contrastThreshold in the sense that, higher edgeThreshold value results in more features, because it basically eleminates features above that threshold not below.
        So, higher threshold results in more features, but once again that doesn't imply more accuracy, we still need to tune our param to give priority to important edges.
        As it's clear from our experiments, low edgeThreshold values yields less features, edgeThreshold=1 is an extreme case which results in no sift descriptors. 
        However, after some point accuracy doesn't improve as threshold gets larger.
        \\~\\
        Finally, sigma parameter is the sigma of the gaussians applied at each octave layers in sift.
        Higher sigma implies more blurring, which would encourage sift to capture features at higher scales
        and lower sigma would encourage sift to capture features at smaller scale. 
        As sigma increases, number of sift descriptors decrease, because blurring kills some details.
        Though, accuracy might seem increasing as sigma increases, but this is due to the fact that I classify none descriptors as a separate class NOT\_FOUND with index -1 in test\_acc dictionary 
        and I've observed that number of elements in NOT\_FOUND increases with sigma which corresponds to less information for that image.
        That's why I've also created another accuracy function which includes NOT\_FOUND as incorrect predictions.
        


    \end{center}

\section{Bag of Features (45 pts)}
    \begin{itemize}
        \item How did you implement BoF? Briefly explain.
        \item Give pseudo-code for obtaining the dictionary.
        \item Give pseudo-code for obtaining BoF representation of an image once the dictionary is formed. 
        \item Put your quantitative results (classification accuracy) regarding 3 different parameter configurations for the BoF pipeline here. Discuss possible reasons for each one's relatively better/worse accuracy. You are suggested to keep $k \leq 1024$ in k-means to keep experiment durations managable. You need to use the best feature extractor you obtained in the previous part together with the same classifier.
    \end{itemize}


\section{Classification (30 pts)}
    \begin{itemize}
        \item Put your quantitative results regarding k-Nearest Neighbor Classifier for k values 16, 32 and 64 by using the best k-means representation and feature extractor. Discuss the effect of these briefly.
        \item What is the accuracy values, and how do you evaluate it? Briefly explain.
        \item Give confusion matrices for classification results of these combinations.
    \end{itemize}

%\section{Your Best Configuration (30 pts)}
    %\begin{itemize}
        %\item You may try different combinations by changing parameters above. Simply give your best accuracy for the validation set. How did you decide to use this configuration?
        %\item Explain your setup for this best accuracy. How can we reproduce your result using your code?
        %\item Visualize confusion matrix for your best classification and briefly interpret the values.
    %\end{itemize}

\section{Additional Comments and References}

    (if there any)





\end{document}
