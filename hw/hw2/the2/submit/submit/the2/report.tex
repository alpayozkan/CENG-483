\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[dvips]{graphicx}
\usepackage{epsfig}
\usepackage{fancybox}
\usepackage{verbatim}
\usepackage{array}
\usepackage{latexsym}
\usepackage{alltt}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[hmargin=3cm,vmargin=5.0cm]{geometry}

\usepackage{epstopdf}
\usepackage{pgfplotstable}
\usepackage{array}
\usepackage{datatool}
\usepackage{siunitx}
\usepackage{csvsimple}


\topmargin=-1.8cm
\addtolength{\textheight}{6.5cm}
\addtolength{\textwidth}{2.0cm}
\setlength{\oddsidemargin}{0.0cm}
\setlength{\evensidemargin}{0.0cm}
\newcommand{\HRule}{\rule{\linewidth}{1mm}}
\newcommand{\kutu}[2]{\framebox[#1mm]{\rule[-2mm]{0mm}{#2mm}}}
\newcommand{\gap}{ \\[1mm] }
\newcommand{\Q}{\raisebox{1.7pt}{$\scriptstyle\bigcirc$}}
\newcommand{\minus}{\scalebox{0.35}[1.0]{$-$}}



\lstset{
    %backgroundcolor=\color{lbcolor},
    tabsize=2,
    language=MATLAB,
    basicstyle=\footnotesize,
    numberstyle=\footnotesize,
    aboveskip={0.0\baselineskip},
    belowskip={0.0\baselineskip},
    columns=fixed,
    showstringspaces=false,
    breaklines=true,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    %frame=single,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    identifierstyle=\ttfamily,
    keywordstyle=\color[rgb]{0,0,1},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
}


\begin{document}

\noindent
\HRule %\\[3mm]
\small
\begin{center}
    \LARGE \textbf{CENG 483} \\[4mm]
    \Large Introduction to Computer Vision \\[4mm]
    \normalsize Fall 2021-2022 \\
    \Large Take Home Exam 2 \\
    \Large Object Recognition \\
    \Large Student Random ID: 2375574 \\
\end{center}
\HRule

\begin{center}
\end{center}
\vspace{-10mm}
\noindent\\ \\ 
Please fill in the sections below only with the requested information. If you have additional things to mention, you can use the last section. Please note that all of the results in this report should be given for the \textbf{validation set}. Also, when you are expected to comment on the effect of a parameter, please make sure to fix other parameters.

\section{Local Features (25 pts)}
    \begin{itemize}
        \item Explain SIFT and Dense-SIFT in your own words. What is the main difference?
        
        \item Put your quantitative results (classification accuracy) regarding 5 values of SIFT and 3 values of Dense-SIFT parameters here. In SIFT change each parameter once while keeping others same and in Dense-SIFT change size of feature extraction region. Discuss the effect of these parameters by using 128 clusters in k-means and 8 nearest neighbors for classification.
        
    \end{itemize}
    
    \vspace*{0.5cm}
    \begin{center}
        \raggedright
        Sift is a keypoint detector that is designed to capture features that are indepent from the scale of the image.
        Its scale invariance property stems from difference of gaussian operations with different kernel size resolutions which captures keypoints at different scales.
        Then we can quantize an orientation for each keypoint and when orientations of all features are described wrt the keypoint orientation, then it'll additionally have rotation invariance alongside scale invariance.
        These are all desired invariances that are needed to select interesting or descriptive points in images in order to identify, match, classify etc.
        Dense-Sift is a kind of modification on top of conventional Sift method. In Sift locations of keypoint descriptors are determined by the default algorithm.
        On the contrary, in Dense-Sift center-positions (x,y) can be fed to Sift algorithm to create descriptors around that point, or equivalently one can partition the image into grid like portions 
        and feed them individually through default Sift which in a way has similar behaviour, resulting in specific sift vectors around that spatial region.
    \end{center}
    
    \vspace*{0.5cm}
        \begin{tabular}{ |p{1.5cm}||p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{2cm}|  }
            \hline
            \multicolumn{6}{|c|}{Sift: Parameters vs Accuracy} \\
            \hline
            Accuracy (1.0) & nfeatures & nOctaveLayers & contrastThreshold & edgeThreshold & sigma \\
            \hline
            % default
            0.17 & 0 & 3 & 0.04 & 10 & 1.6 \\
            \hline
            \hline
            % nfeatures
            0.10 & 1 & 3 & 0.04 & 10 & 1.6 \\
            0.18 & 20 & 3 & 0.04 & 10 & 1.6 \\
            0.16 & 100 & 3 & 0.04 & 10 & 1.6 \\
            \hline
            \hline
            % nOctaveLayers
            0.19 & 0 & 1 & 0.04 & 10 & 1.6 \\
            0.16 & 0 & 5 & 0.04 & 10 & 1.6 \\
            0.17 & 0 & 7 & 0.04 & 10 & 1.6 \\
            \hline
            \hline
            % contrastThreshold
            0.0 & 0 & 3 & 1 & 10 & 1.6 \\
            0.16 & 0 & 3 & 0.1 & 10 & 1.6 \\
            0.17 & 0 & 3 & 0.01 & 10 & 1.6 \\
            \hline
            \hline
            % edgeThreshold
            0.0 & 0 & 3 & 0.04 & 1 & 1.6 \\
            0.19 & 0 & 3 & 0.04 & 50 & 1.6 \\
            0.19 & 0 & 3 & 0.04 & 100 & 1.6 \\
            \hline
            \hline
            % sigma
            0.0 & 0 & 3 & 0.04 & 10 & 0.1 \\
            0.15 & 0 & 3 & 0.04 & 10 & 1 \\
            0.17 & 0 & 3 & 0.04 & 10 & 2 \\
            0.17 & 0 & 3 & 0.04 & 10 & 3 \\
            0.0 & 0 & 3 & 0.04 & 10 & 10 \\
            \hline


            \hline
        \end{tabular}

        


    \begin{center}
        \raggedright
        In this experiment, k-value in kmeans is set to 128 cluster and k value of k\_nn neearest neighbor is set to 8 as required.
        By changing the parameters of SIFT, I've obtained significant changes on the accuracy of our classifier.
        \\~\\
        Firstly, nfeatures determines the number of top features in terms of local contrast scores to keep in the sift descriptor. 
        Default nfeatures value takes all of the features in the descriptor. When nfeatures is set very low like 1, I've observed that accuracy drops drastically
        which makes sense because we are dropping discriminative sift vectors and this information loss has a cost on the overall accuracy.
        However, nfeatures=20 or 100, more or less yields the same accuracy which probably means that top 20 sift vectors were discriminative enough to make 
        classification equivalent to the default param that is used in sift.
        \\~\\
        Secondly, nOctaveLayers are number of difference of gaussian layers which helps capturing features at varying scales.
        If we have more nOctaveLayers then that would be more features at varying scales but with exponentially (with order of $k^{2}$) more blurring.
        This creates new features at very high scale due to higher blurring as a result of increased nOctaveLayers value.
        As far as I understand, in sift they have probably chosen the elbow value which should be somewhere around 3, because after/smaller than 3 it starts to decline very sharply.
        In addition, having less nOctaveLayers might be disadvantageous too depending on the problem and image properties, hence I think 3 would be a better or less risky choice when compared to 1,2 which results
        in less scale invariant descriptors.
        \\~\\
        Thirdly, contrastThreshold parameter filters weak feature below a threshold value measured in contrast.
        When it's set higher, we would get less features and when it's set lower, we would get more features.
        However, more features don't always imply more accuracy, some redundant features can hurt our performance in classification as we'll see.
        I've observed that increasing contrastThreshold, results in less sift descriptors. For instance, contrastThreshold=1 results no sift vector, because it's too high and contrastThreshold=0.001 results in most number of sift descriptors.
        ContrastThreshold is a hyperparameter that we should tune for our problem and in our case 0.04 has resulted in the best accuracy, because it has discarded less important features and emphasized on more important features.
        \\~\\
        Fourtly, edgeThreshold is a hyperparameter that basically filters edge like features, its meaning is slightly different than contrastThreshold in the sense that, higher edgeThreshold value results in more features, because it basically eleminates features above that threshold not below.
        So, higher threshold results in more features, but once again that doesn't imply more accuracy, we still need to tune our param to give priority to important edges.
        As it's clear from our experiments, low edgeThreshold values yields less features, edgeThreshold=1 is an extreme case which results in no sift descriptors. 
        However, after some point accuracy doesn't improve as threshold gets larger.
        \\~\\
        Finally, sigma parameter is the sigma of the gaussians applied at each octave layers in sift.
        Higher sigma implies more blurring, which would encourage sift to capture features at higher scales
        and lower sigma would encourage sift to capture features at smaller scale. 
        As sigma increases, number of sift descriptors decrease, because blurring kills some details.
        Also, low sigma values have low accuracies, because we capture features at lower scales more frequently.
        That's why we need to form a balance for the problem by tuning the hyperparameter.
       
        % Dense-Sift
        \vspace*{0.5cm}
        \begin{tabular}{ |p{1.5cm}||p{3cm}|p{3cm}|p{3cm}| }
            \hline
            \multicolumn{4}{|c|}{Dense-Sift: Parameters vs Accuracy (default sift parameters)} \\
            \hline
            Accuracy (1.0) & grid\_size & keypoint\_diameter & offset \\
            \hline
            0.31 & 4 & 4 & 4 \\ % default
            0.27 & 8 & 4 & 4 \\ % default
            0.20 & 16 & 4 & 4 \\ % 2x2
            0.12 & 32 & 4 & 4 \\ % 1x1
            \hline
            \hline
            0.07 & 8 & 0.1 & 4 \\ 
            0.14 & 8 & 1 & 4 \\ 
            0.20 & 8 & 10 & 4 \\ 
            \hline
            \hline
            0.25 & 8 & 4 & 0 \\ 
            0.27 & 8 & 4 & 2 \\ 
            0.25 & 8 & 4 & 8 \\ 
            \hline
        \end{tabular}
        
        \vspace*{0.5cm}

        In Dense-Sift, it's mentioned only grid-size parameter, but in order to extract key points at certain locations, we have to pass a keypoint diameter.
        Hence, I've parametrised over grid-size, keypoint-diameter, and also an offset value to skip edge pixels and focus on inner part of the image as part of the experimentation.
        \\~\\
        Firstly, grid-size parameter adjusts spatiality of our sift descriptors, as expected as spatiality decreases we get less accuracy.
        Grid-size 4 has resulted in the best accuracy, whereas grid-size 32 with only one keypoint has very poor accuracy.
        However, if we have too little grid-size s like 1x1, I would expect not a very good accuracy, because too much unnecessary keypoints would overwhelm our classification leaving less emphasis on important keypoints.
        \\~\\
        Secondaly, keypoint-diameter adjusts the radius of the keypoint at that (x,y) location. In other words, it corresponds to the diameter of the keypoint circles.
        As it's illustrated in our experiment that too low and too high keypoint-diameters result in less accuracy. This parameter is also in a way correlated with grid-size, because as our grid-sizes decrease
        our key-point diameter should also be decreased, because otherwise it wouldn't be meaning to have large circles overflowing smaller grids.
        \\~\\
        Thirdly, offset value is how much we skip from edges of an image.
        It is a reasonable assumption that some pixels from edge are not that interesting and we focus more on inner part of the image.
        Our observations make sense, when we increase offset too much we get less accuracy due to information loss, however by tuning the parameter we can increase accuracy with around 1-2 points.

    \end{center}

\section{Bag of Features (45 pts)}
    \begin{itemize}
        \item How did you implement BoF? Briefly explain.
        \item Give pseudo-code for obtaining the dictionary.
        \item Give pseudo-code for obtaining BoF representation of an image once the dictionary is formed. 
        \item Put your quantitative results (classification accuracy) regarding 3 different parameter configurations for the BoF pipeline here. Discuss possible reasons for each one's relatively better/worse accuracy. You are suggested to keep $k \leq 1024$ in k-means to keep experiment durations managable. You need to use the best feature extractor you obtained in the previous part together with the same classifier.
    \end{itemize}

    \begin{center}
        \raggedright
        I've implemented BoF through several steps. Firstly, I've iterated over all images and extracted their sift vectors/descriptors as in the from (m, 128) where m is the number of detected keypoints and 128 is the length of the sift vector.
        If there was no sift-vector which happened rarely, I've added a null vector with all zeros in the form of (1,128) to describe that case, in order to comply with the general BoF model.
        Then, I've saved all of these sift descriptors for each image which we now the class due to training set. Then I've stacked all of these vectors of images all together, and applied kmeans with k=128 and iters=15.
        Also, I've sampled descriptor vectors from this stack, because number of descriptors are too large sometimes and it crashes the kmeans algorithm, probably due to ram memory, especially as grid-size gets smaller in dense-sift.
        That gives me a codebook of size (128,128) where we have 128 many centroids of sift vectors of size 128.
        I've implemented my own knn algorithm to get k-nearest neighbors of sift descriptors.
        Also, I used my own knn implementation to get 1-nn for bag of words representation calculation, where I assign sift descriptors of an image to its neares centroid in the codebook 
        and I keep a statistics or histogram of these codebooks/bins for each image.
        Then I normalize these histogram such that each image codebook histogram sums to 1, aka l-1 norm.
        In the second phase, I calculate accuracy over test-set and I apply the same stages as I did in training namely
        extracting sift vectors, assigning vectors to centroids, and calculation normalized bow representations for each test image.
        Then, I run knn over normalized test image bow representation wrt normalized training image bow representation with k=8 as required to get 8 neares neighbor 
        and I take the dominant class out of these votes to make a prediction. Finally, I calculate my test accuracy since I've the labels.
        \\~\\
        Pseudo-code for dictionary: \\
        1. Obtain sift descriptor vectors for each image in the set. \\
        2. Stack sift descriptor vectors all-together. \\
        3. Run kmeans over the descriptor stack with parameters k and iters. \\
        4. Returns codebook/dictionary of shape (k,128) including k-centroids of 128-d sift vectors.
        \\~\\
        Pseudo-code for BoF representation: \\
        1. Apply for each image in the set/ \\
        2. Calculate knn with k=1 (same as 1-nearest neighbor) between codebook (pre-calculated dictionary of (kmeans\_k,128)) and image sift descriptors. \\
        3. Assign each image descriptor vector to its closest centroid in codebook \\
        4. Count the number of occurances of each codebook vector for that image (histogram) \\
        5. Normalize histogram representation such that it sums to 1. \\
        6. Returns normalized BoF representation for that image
    \end{center}

    \vspace*{0.5cm}
        \begin{tabular}{ |p{1.5cm}||p{3cm}|p{3cm}|p{3cm}| }
            \hline
            \multicolumn{3}{|c|}{Dense-Sift: BoF Parameters vs Accuracy (default sift parameters)} \\
            \hline
            Accuracy (1.0)  & kmeans\_k & kmeans\_iters\\
            \hline
            0.31 & 128 & 15 \\ % default
            \hline
            0.31 & 256 & 30 \\
            \hline
            0.33 & 256 & 15 \\
            \hline
            0.27 & 64 & 15 \\
            \hline
            0.29 & 32 & 15 \\
            \hline
        \end{tabular}
       
        \begin{center}
            \raggedright
            The feature extractor for these results are from Dense-Sift with default sift parameters and grid-size=8, keypoint-diamater=4, offset=4 which is used a baseline comparison model. Knn\_k is also fixed as 8 as in the previous experiments.
            In this experiment, we measure how much accuracy changes when we change kmeans paraemeters
            for BoF pipeline. I've parameterised this pipeline over kmeans\_k and kmeans\_iters.
            \\~\\
            Increasing iterations helps centroids to converge better vectors that minimize distance more.
            Hence it has a potential to represent this vector space more generically in this case.
            However, it increases the computational complexity of our model and it takes too much time to obtain experiment results. 
            That's why I couldn't exceed iterations more than 30.
            As we increase kmeans\_k it is reasonable to increase kmeans\_iters because the dimensionality increases and probably we would need more
            iterations to converge to a proper centroid.
            \\~\\
            As it's clear from the observations decreasing kmeans\_k below 128 has resulted in less accuracy around 0.27 and 0.29, because codebook dimensionality has decreased
            and we have less vectors or bins that represents bow of images whereas
            it was around 0.30-0.31 with kmeans\_k=128. Increasing kmeans\_k can give us better accuracy, because
            we will increase our features bins in bow representation which can help us in classification.
            As it's clear from our results that we got almost the same accuracy with kmeans\_k=256.
            However, I would prefer having lower kmeans\_k, because it's computationally much more feasible.
            Moreover, increasing kmeans\_k too much would be too overkill and
            we would get too sparse representation which would mitigate the learning process.
            \\~\\
            One interesting observation is that why kmeans\_k=256, kmeans\_iters=15 is higher than kmeans\_k=128, kmeans\_iters=30.
            I think this can be due to random processes in the pipline namely random sampling from descriptor stack and also kmeans algorithm which
            starts from a random initialization. Hence, 1-2 points off accuracy might differ. It's reasonable to pick one of these models as the best ones.

        \end{center}

\section{Classification (30 pts)}
    \begin{itemize}
        \item Put your quantitative results regarding k-Nearest Neighbor Classifier for k values 16, 32 and 64 by using the best k-means representation and feature extractor. Discuss the effect of these briefly.
        \item What is the accuracy values, and how do you evaluate it? Briefly explain.
        \item Give confusion matrices for classification results of these combinations.
    \end{itemize}

    \vspace*{0.5cm}
        \begin{tabular}{ |p{1.5cm}||p{3cm}|p{3cm}| }
            \hline
            \multicolumn{2}{|c|}{Dense-Sift: KNN Parameters vs Accuracy (default sift parameters)} \\
            \hline
            Accuracy (1.0)  & knn\_k\\
            \hline
            0.28 & 4  \\ % default
            \hline
            0.31 & 8  \\ % default
            \hline
            0.30 & 16  \\ % default
            \hline
            0.29 & 32  \\ % default
            \hline
            0.29 & 64  \\ % default
            \hline
        \end{tabular}
        
        \begin{center}
            \raggedright
            I've used the best kmeans configuration that I've calculated previously in terms of both accuracy and computational feasibility which
            is kmean\_k=128, kmeans\_iters=15 and also the best sift extractor same as part-2.
            \\~\\
            First of all, my expectation is that increasing k value of knn would increase accuracy untill some point and then it should decrease the accuracy.
            Because we should give right of vote to important and sufficiently many matchings.
            If k is low, then it would be biased or not democratic enough and also from the other perspective giving voting right to lots of matchings wouldn't be reliable in the sense that
            too far away matchings will have the same effect as the closer ones while classifying.
            My observations imply that knn\_k values are high after 16, gradually accuracy decreases from that point on.
            \\~\\
            Accuracy values in my implementation corresponds to total number of correct predictions divided by total number of predictions (which is equivalent to total number of observations).
            First, I calculate class based accuracies how much of a given class, we can predict the correct label, and then
            I mean this to get the total accuracy for the problem. Some classes have accuracies above the average, because they are easy to classify, but
            whereas some classes have very poor accuracy from my observation.
            \\~\\
            Confusion matrices are attached to the report, I couldn't put them under this section, because it was too large and didn't fit.
            Tables 1-5 below the report correspond to confusion matrices of [4,8,16,32,64] knn\_k values.
            Classes are labeled as [0-14] both for columns correspond to predicted class and where rows correspond to actual class (unfortunately I couldn't put row labels), entry [0,0] corresponds to upper left corner of the matrix as conventional index [0,0]. 
            For ex, for KNN\_4[0,0] = 44, KNN\_4[0,1] = 13 (actual class 0, predicted 1 which occured total of 13 times). Class indices: {'apple': 0, 'aquarium\_fish': 1, 'beetle': 2, 'camel': 3, 'crab': 4, 'cup': 5, 'elephant': 6, 'flatfish': 7, 'lion': 8, 'mushroom': 9, 'orange': 10, 'pear': 11, 'road': 12, 'skyscraper': 13, 'woman': 14}.


        \end{center}

        \begin{table}
            \centering
            \caption{KNN\_4}
            \pgfplotstabletypeset[
                column type=l,
                every head row/.style={before row=\hline,after row=\hline},
                every last row/.style={after row=\hline},
                every first column/.style={column type/.add={|}{}},
                every last column/.style={column type/.add={}{|}},
               fixed zerofill,
               precision=0,
               columns={0,1,2,3,4,5,6,7,8,9,10,11,12,13,14},
               col sep=space,
               ]{confusion/knn_4.csv}
               \centering
            \caption{KNN\_8}
            \pgfplotstabletypeset[
                column type=l,
                every head row/.style={before row=\hline,after row=\hline},
                every last row/.style={after row=\hline},
                every first column/.style={column type/.add={|}{}},
                every last column/.style={column type/.add={}{|}},
               fixed zerofill,
               precision=0,
               columns={0,1,2,3,4,5,6,7,8,9,10,11,12,13,14},
               col sep=space,
               ]{confusion/knn_8.csv}
        \end{table}

        \begin{table}
            \centering
            \caption{KNN\_16}
            \pgfplotstabletypeset[
                column type=l,
                every head row/.style={before row=\hline,after row=\hline},
                every last row/.style={after row=\hline},
                every first column/.style={column type/.add={|}{}},
                every last column/.style={column type/.add={}{|}},
               fixed zerofill,
               precision=0,
               columns={0,1,2,3,4,5,6,7,8,9,10,11,12,13,14},
               col sep=space,
               ]{confusion/knn_16.csv}
               \centering
            \caption{KNN\_32}
            \pgfplotstabletypeset[
                column type=l,
                every head row/.style={before row=\hline,after row=\hline},
                every last row/.style={after row=\hline},
                every first column/.style={column type/.add={|}{}},
                every last column/.style={column type/.add={}{|}},
               fixed zerofill,
               precision=0,
               columns={0,1,2,3,4,5,6,7,8,9,10,11,12,13,14},
               col sep=space,
               ]{confusion/knn_32.csv}
        \end{table}
        
        
        \begin{table}
            \centering
            \caption{KNN\_64}
            \pgfplotstabletypeset[
                column type=l,
                every head row/.style={before row=\hline,after row=\hline},
                every last row/.style={after row=\hline},
                every first column/.style={column type/.add={|}{}},
                every last column/.style={column type/.add={}{|}},
               fixed zerofill,
               precision=0,
               columns={0,1,2,3,4,5,6,7,8,9,10,11,12,13,14},
               col sep=space,
               ]{confusion/knn_64.csv}
        \end{table}

%\section{Your Best Configuration (30 pts)}
    %\begin{itemize}
        %\item You may try different combinations by changing parameters above. Simply give your best accuracy for the validation set. How did you decide to use this configuration?
        %\item Explain your setup for this best accuracy. How can we reproduce your result using your code?
        %\item Visualize confusion matrix for your best classification and briefly interpret the values.
    %\end{itemize}

\section{Additional Comments and References}

    (if there any)





\end{document}
