\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[dvips]{graphicx}
\usepackage{epsfig}
\usepackage{fancybox}
\usepackage{verbatim}
\usepackage{array}
\usepackage{latexsym}
\usepackage{alltt}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[hmargin=3cm,vmargin=5.0cm]{geometry}
\usepackage{epstopdf}
\topmargin=-1.8cm
\addtolength{\textheight}{6.5cm}
\addtolength{\textwidth}{2.0cm}
\setlength{\oddsidemargin}{0.0cm}
\setlength{\evensidemargin}{0.0cm}
\newcommand{\HRule}{\rule{\linewidth}{1mm}}
\newcommand{\kutu}[2]{\framebox[#1mm]{\rule[-2mm]{0mm}{#2mm}}}
\newcommand{\gap}{ \\[1mm] }
\newcommand{\Q}{\raisebox{1.7pt}{$\scriptstyle\bigcirc$}}
\newcommand{\minus}{\scalebox{0.35}[1.0]{$-$}}



\lstset{
    %backgroundcolor=\color{lbcolor},
    tabsize=2,
    language=MATLAB,
    basicstyle=\footnotesize,
    numberstyle=\footnotesize,
    aboveskip={0.0\baselineskip},
    belowskip={0.0\baselineskip},
    columns=fixed,
    showstringspaces=false,
    breaklines=true,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    %frame=single,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    identifierstyle=\ttfamily,
    keywordstyle=\color[rgb]{0,0,1},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
}


\begin{document}

\noindent
\HRule %\\[3mm]
\small
\begin{center}
    \LARGE \textbf{CENG 483} \\[4mm]
    \Large Introduction to Computer Vision \\[4mm]
    \normalsize Fall 2021-2022 \\
    \Large Take Home Exam 2 \\
    \Large Object Recognition \\
    \Large Student Random ID: \\
\end{center}
\HRule

\begin{center}
\end{center}
\vspace{-10mm}
\noindent\\ \\ 
Please fill in the sections below only with the requested information. If you have additional things to mention, you can use the last section. Please note that all of the results in this report should be given for the \textbf{validation set}. Also, when you are expected to comment on the effect of a parameter, please make sure to fix other parameters.

\section{Local Features (25 pts)}
    \begin{itemize}
        \item Explain SIFT and Dense-SIFT in your own words. What is the main difference?
        
        \item Put your quantitative results (classification accuracy) regarding 5 values of SIFT and 3 values of Dense-SIFT parameters here. In SIFT change each parameter once while keeping others same and in Dense-SIFT change size of feature extraction region. Discuss the effect of these parameters by using 128 clusters in k-means and 8 nearest neighbors for classification.
        
    \end{itemize}
    
    \vspace*{0.5cm}
    \begin{center}
        \raggedright
        Sift is a keypoint detector that is designed to capture features that are indepent from the scale of the image.
        Its scale invariance property stems from difference of gaussian operations with different kernel size resolutions which captures keypoints at different scales.
        Then we can quantize an orientation for each keypoint and when orientations of all features are described wrt the keypoint orientation, then it'll additionally have rotation invariance alongside scale invariance.
        These are all desired invariances that are needed to select interesting or descriptive points in images in order to identify, match, classify etc.
        Dense-Sift is a kind of modification on top of conventional Sift method. In Sift locations of keypoint descriptors are determined by the default algorithm.
        On the contrary, in Dense-Sift center-positions (x,y) can be fed to Sift algorithm to create descriptors around that point, or equivalently one can partition the image into grid like portions 
        and feed them individually through default Sift which in a way has similar behaviour, resulting in specific sift vectors around that spatial region.
    \end{center}
    
    \vspace*{0.5cm}
        \begin{tabular}{ |p{1.5cm}||p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{2cm}|  }
            \hline
            \multicolumn{6}{|c|}{Sift: Parameters vs Accuracy (default parameters)} \\
            \hline
            Accuracy (1.0) & nfeatures & nOctaveLayers & contrastThreshold & edgeThreshold & sigma \\
            \hline
            0.17 & 0 & 3 & 0.04 & 10 & 1.6 \\
            \hline
        \end{tabular}
    
        \vspace*{0.5cm}
    \begin{tabular}{ |p{1.5cm}||p{2cm}| }
        \hline
        \multicolumn{2}{|c|}{Sift: Parameters vs Accuracy (other params default)} \\
        \hline
        Accuracy (1.0) & nfeatures \\
        \hline
        0.17 & 0 \\
        \hline
        0.17 & 100 \\
        \hline
        0.17 & 20 \\
        \hline
        0.16 & 10 \\
        \hline
        0.10 & 1 \\
        \hline
    \end{tabular}

    \vspace*{0.5cm}
    \begin{tabular}{ |p{1.5cm}||p{2cm}| }
        \hline
        \multicolumn{2}{|c|}{Sift: Parameters vs Accuracy (other params default)} \\
        \hline
        Accuracy (1.0) & nOctaveLayers \\
        \hline
        - & 1 \\
        \hline
        0.17 & 3 \\
        \hline
        0.18 & 5 \\
        \hline
        - & 10 \\
        \hline
        - & 20 \\
        \hline
        - & 30 \\
        \hline
    \end{tabular}

    \vspace*{0.5cm}
        \begin{tabular}{ |p{1.5cm}||p{3cm}|  }
            \hline
            \multicolumn{2}{|c|}{Dense-Sift: Parameters vs Accuracy} \\
            \hline
            Accuracy (1.0) & Grid\_Size \\
            \hline
            - & - \\
            \hline
            - & - \\
            \hline
            - & - \\
            \hline
        \end{tabular}

    \begin{center}
        \raggedright
        In this experiment, k-value in kmeans is set to 128 cluster and k value of k\_nn neearest neighbor is set to 8 as required.
        By changing the parameters of SIFT, I've obtained significant changes on the accuracy of our classifier.
        \\
        Firstly, nfeatures determines the number of top features in terms of local contrast scores to keep in the sift descriptor. 
        Default nfeatures value takes all of the features in the descriptor. When nfeatures is set very low like 1, I've observed that accuracy drops drastically
        which makes sense because we are dropping discriminative sift vectors and this information loss has a cost on the overall accuracy.
        However, nfeatures=20 or 100, more or less yields the same accuracy which probably means that top 20 sift vectors were discriminative enough to make 
        classification equivalent to the default param that is used in sift.
        \\
        Secondly, nOctaveLayers are number of difference of gaussian layers which helps capturing features at varying scales.
        If we have more nOctaveLayers then that would be more features at varying scales which means we have more information to describe the class.
        As, it's clear from the observations increasing nOctaveLayers yields better accuracy for this problem.
        However, it brings more computational costs.

    \end{center}

\section{Bag of Features (45 pts)}
    \begin{itemize}
        \item How did you implement BoF? Briefly explain.
        \item Give pseudo-code for obtaining the dictionary.
        \item Give pseudo-code for obtaining BoF representation of an image once the dictionary is formed. 
        \item Put your quantitative results (classification accuracy) regarding 3 different parameter configurations for the BoF pipeline here. Discuss possible reasons for each one's relatively better/worse accuracy. You are suggested to keep $k \leq 1024$ in k-means to keep experiment durations managable. You need to use the best feature extractor you obtained in the previous part together with the same classifier.
    \end{itemize}


\section{Classification (30 pts)}
    \begin{itemize}
        \item Put your quantitative results regarding k-Nearest Neighbor Classifier for k values 16, 32 and 64 by using the best k-means representation and feature extractor. Discuss the effect of these briefly.
        \item What is the accuracy values, and how do you evaluate it? Briefly explain.
        \item Give confusion matrices for classification results of these combinations.
    \end{itemize}

%\section{Your Best Configuration (30 pts)}
    %\begin{itemize}
        %\item You may try different combinations by changing parameters above. Simply give your best accuracy for the validation set. How did you decide to use this configuration?
        %\item Explain your setup for this best accuracy. How can we reproduce your result using your code?
        %\item Visualize confusion matrix for your best classification and briefly interpret the values.
    %\end{itemize}

\section{Additional Comments and References}

    (if there any)





\end{document}
